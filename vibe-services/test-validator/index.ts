#!/usr/bin/env node

/**
 * VibeCoding Test Validator MCP Server
 * æ•´åˆ Prompt ç®¡ç†ç³»çµ±çš„æ¸¬è©¦é©—è­‰æœå‹™
 */

import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ErrorCode,
  ListToolsRequestSchema,
  McpError,
} from '@modelcontextprotocol/sdk/types.js';
import { existsSync, readFileSync } from 'fs';
import { join } from 'path';

// å°å…¥ Prompt ç®¡ç†ç³»çµ±
import { 
  buildMCPServicePrompt, 
  ServiceId, 
  DevelopmentPhase,
} from '../../src/utils/prompt-manager.js';

interface TestResult {
  testFile: string;
  status: 'passed' | 'failed' | 'skipped';
  duration: number;
  assertions: number;
  passed: number;
  failed: number;
  errors: string[];
}

interface TestSummary {
  totalTests: number;
  passed: number;
  failed: number;
  skipped: number;
  duration: number;
  coverage: CoverageInfo;
  results: TestResult[];
}

interface CoverageInfo {
  lines: { total: number; covered: number; percentage: number };
  functions: { total: number; covered: number; percentage: number };
  branches: { total: number; covered: number; percentage: number };
  statements: { total: number; covered: number; percentage: number };
}

interface PerformanceResult {
  testSuite: string;
  iterations: number;
  avgTime: number;
  minTime: number;
  maxTime: number;
  throughput: number;
  memoryUsage: number;
}

class VibeTestValidator {
  private currentSession: string | null = null;

  constructor() {
    // åˆå§‹åŒ– Prompt ç³»çµ±
    this.initializePromptSystem();
  }

  /**
   * åˆå§‹åŒ– Prompt ç®¡ç†ç³»çµ±
   */
  private async initializePromptSystem(): Promise<void> {
    try {
      await buildMCPServicePrompt(
        ServiceId.TEST_VALIDATOR,
        DevelopmentPhase.VALIDATION,
        {
          capabilities: ['test-execution', 'coverage-analysis', 'performance-testing', 'report-generation'],
          supportedFrameworks: ['jest', 'mocha', 'cypress', 'playwright', 'vitest'],
          testTypes: ['unit', 'integration', 'e2e', 'performance']
        }
      );
      
      console.error('[Test Validator] Prompt system initialized successfully');
    } catch (error) {
      console.error('[Test Validator] Failed to initialize prompt system:', error);
    }
  }

  /**
   * é–‹å§‹æœƒè©±
   */
  async startSession(): Promise<{ sessionId: string; message: string }> {
    this.currentSession = `test-session-${Date.now()}`;
    
    console.error(`[Test Validator] Session started: ${this.currentSession}`);
    
    return {
      sessionId: this.currentSession,
      message: 'ğŸ§ª Test Validator æœå‹™å·²å•Ÿå‹•ï¼å¯ä»¥é–‹å§‹åŸ·è¡Œæ¸¬è©¦ã€ç”Ÿæˆå ±å‘Šæˆ–é€²è¡Œæ€§èƒ½æ¸¬è©¦ã€‚'
    };
  }

  /**
   * åŸ·è¡Œæ¸¬è©¦
   */
  async runTests(
    projectPath: string,
    testType: 'unit' | 'integration' | 'e2e' | 'all' = 'all',
    pattern?: string,
    watch: boolean = false
  ): Promise<TestSummary> {
    console.error(`[Test Validator] Running ${testType} tests${pattern ? ` with pattern: ${pattern}` : ''}`);
    
    if (!existsSync(projectPath)) {
      throw new Error(`Project path does not exist: ${projectPath}`);
    }

    const testFramework = this.detectTestFramework(projectPath);
    console.error(`[Test Validator] Using test framework: ${testFramework}`);

    if (watch) {
      console.error('[Test Validator] Running in watch mode');
    }

    const results = await this.executeTests(projectPath, testType, pattern, testFramework);
    
    return results;
  }

  /**
   * æª¢æ¸¬æ¸¬è©¦æ¡†æ¶
   */
  private detectTestFramework(projectPath: string): string {
    const packageJsonPath = join(projectPath, 'package.json');
    
    if (existsSync(packageJsonPath)) {
      try {
        const packageJson = JSON.parse(readFileSync(packageJsonPath, 'utf-8'));
        const dependencies = { ...packageJson.dependencies, ...packageJson.devDependencies };
        
        if (dependencies.jest) return 'jest';
        if (dependencies.vitest) return 'vitest';
        if (dependencies.mocha) return 'mocha';
        if (dependencies.cypress) return 'cypress';
        if (dependencies.playwright) return 'playwright';
      } catch (error) {
        console.error('[Test Validator] Failed to parse package.json:', error);
      }
    }

    return 'jest';
  }

  /**
   * åŸ·è¡Œæ¸¬è©¦
   */
  private async executeTests(projectPath: string, testType: string, pattern: string | undefined, framework: string): Promise<TestSummary> {
    const startTime = Date.now();
    
    // æ¨¡æ“¬æ¸¬è©¦åŸ·è¡Œ
    const results: TestResult[] = [];
    
    switch (testType) {
      case 'unit':
        results.push(...this.generateMockTestResults('unit', 15, 0.8));
        break;
      case 'integration':
        results.push(...this.generateMockTestResults('integration', 8, 0.9));
        break;
      case 'e2e':
        results.push(...this.generateMockTestResults('e2e', 5, 0.85));
        break;
      case 'all':
        results.push(
          ...this.generateMockTestResults('unit', 15, 0.8),
          ...this.generateMockTestResults('integration', 8, 0.9),
          ...this.generateMockTestResults('e2e', 5, 0.85)
        );
        break;
    }

    const totalTests = results.reduce((sum, result) => sum + result.assertions, 0);
    const passed = results.reduce((sum, result) => sum + result.passed, 0);
    const failed = results.reduce((sum, result) => sum + result.failed, 0);
    const skipped = results.filter(result => result.status === 'skipped').length;
    const duration = Date.now() - startTime;

    const coverage = this.generateMockCoverage();

    return {
      totalTests,
      passed,
      failed,
      skipped,
      duration,
      coverage,
      results
    };
  }

  /**
   * ç”Ÿæˆæ¨¡æ“¬æ¸¬è©¦çµæœ
   */
  private generateMockTestResults(testType: string, count: number, successRate: number): TestResult[] {
    const results: TestResult[] = [];
    
    for (let i = 1; i <= count; i++) {
      const assertions = Math.floor(Math.random() * 10) + 5;
      const passed = Math.floor(assertions * successRate);
      const failed = assertions - passed;
      const status = failed > 0 ? 'failed' : 'passed';
      
      results.push({
        testFile: `${testType}/test-${i}.spec.js`,
        status,
        duration: Math.floor(Math.random() * 1000) + 100,
        assertions,
        passed,
        failed,
        errors: failed > 0 ? [`Assertion failed in test ${i}`] : []
      });
    }
    
    return results;
  }

  /**
   * ç”Ÿæˆæ¨¡æ“¬è¦†è“‹ç‡
   */
  private generateMockCoverage(): CoverageInfo {
    return {
      lines: { total: 1000, covered: 850, percentage: 85 },
      functions: { total: 120, covered: 108, percentage: 90 },
      branches: { total: 200, covered: 160, percentage: 80 },
      statements: { total: 950, covered: 808, percentage: 85 }
    };
  }

  /**
   * ç”Ÿæˆæ¸¬è©¦å ±å‘Š
   */
  async generateTestReport(
    projectPath: string,
    format: 'html' | 'json' | 'xml' | 'lcov' = 'html',
    includeMetrics: boolean = true
  ): Promise<{
    success: boolean;
    reportPath: string;
    format: string;
    size: number;
    metrics: Record<string, any>;
  }> {
    console.error(`[Test Validator] Generating test report in ${format} format`);
    
    if (!existsSync(projectPath)) {
      throw new Error(`Project path does not exist: ${projectPath}`);
    }

    const reportPath = join(projectPath, 'test-reports', `report.${format}`);
    const size = Math.floor(Math.random() * 1000000) + 100000;

    const metrics = includeMetrics ? {
      testSuites: 12,
      totalTests: 156,
      passedTests: 142,
      failedTests: 14,
      coverage: 85.2,
      duration: 45.6,
      performance: {
        avgResponseTime: 250,
        maxMemoryUsage: '256MB',
        cpuUtilization: 45
      }
    } : {};

    return {
      success: true,
      reportPath,
      format,
      size,
      metrics
    };
  }

  /**
   * é©—è­‰è¦†è“‹ç‡
   */
  async validateCoverage(
    projectPath: string,
    threshold?: {
      lines?: number;
      functions?: number;
      branches?: number;
      statements?: number;
    },
    failOnThreshold: boolean = false
  ): Promise<{
    passed: boolean;
    coverage: CoverageInfo;
    thresholds: Record<string, { required: number; actual: number; passed: boolean }>;
    recommendations: string[];
  }> {
    console.error('[Test Validator] Validating coverage thresholds');
    
    if (!existsSync(projectPath)) {
      throw new Error(`Project path does not exist: ${projectPath}`);
    }

    const coverage = this.generateMockCoverage();
    const defaultThreshold = { lines: 80, functions: 80, branches: 70, statements: 80 };
    const appliedThreshold = { ...defaultThreshold, ...threshold };

    const thresholds = {
      lines: {
        required: appliedThreshold.lines,
        actual: coverage.lines.percentage,
        passed: coverage.lines.percentage >= appliedThreshold.lines
      },
      functions: {
        required: appliedThreshold.functions,
        actual: coverage.functions.percentage,
        passed: coverage.functions.percentage >= appliedThreshold.functions
      },
      branches: {
        required: appliedThreshold.branches,
        actual: coverage.branches.percentage,
        passed: coverage.branches.percentage >= appliedThreshold.branches
      },
      statements: {
        required: appliedThreshold.statements,
        actual: coverage.statements.percentage,
        passed: coverage.statements.percentage >= appliedThreshold.statements
      }
    };

    const allPassed = Object.values(thresholds).every(t => t.passed);
    const recommendations = this.generateCoverageRecommendations(thresholds);

    if (failOnThreshold && !allPassed) {
      throw new Error('Coverage thresholds not met');
    }

    return {
      passed: allPassed,
      coverage,
      thresholds,
      recommendations
    };
  }

  /**
   * ç”Ÿæˆè¦†è“‹ç‡å»ºè­°
   */
  private generateCoverageRecommendations(thresholds: Record<string, { required: number; actual: number; passed: boolean }>): string[] {
    const recommendations = [];

    Object.entries(thresholds).forEach(([metric, data]) => {
      if (!data.passed) {
        const gap = data.required - data.actual;
        recommendations.push(`æé«˜ ${metric} è¦†è“‹ç‡ ${gap.toFixed(1)}% ä»¥é”åˆ° ${data.required}% é–¾å€¼`);
      }
    });

    if (recommendations.length === 0) {
      recommendations.push('è¦†è“‹ç‡è¡¨ç¾è‰¯å¥½ï¼Œç¹¼çºŒä¿æŒé«˜è³ªé‡æ¸¬è©¦');
    } else {
      recommendations.push('è€ƒæ…®æ·»åŠ æ›´å¤šæ¸¬è©¦ç”¨ä¾‹è¦†è“‹æœªæ¸¬è©¦çš„ä»£ç¢¼');
      recommendations.push('æª¢æŸ¥ä¸¦æ”¹é€²ç¾æœ‰æ¸¬è©¦çš„æ–·è¨€å®Œæ•´æ€§');
    }

    return recommendations;
  }

  /**
   * æ€§èƒ½æ¸¬è©¦
   */
  async performanceTest(
    projectPath: string,
    testSuite?: string,
    iterations: number = 100,
    warmup: boolean = true
  ): Promise<{
    results: PerformanceResult[];
    summary: {
      totalSuites: number;
      avgThroughput: number;
      maxMemoryUsage: number;
      recommendations: string[];
    };
  }> {
    console.error(`[Test Validator] Running performance tests with ${iterations} iterations`);
    
    if (!existsSync(projectPath)) {
      throw new Error(`Project path does not exist: ${projectPath}`);
    }

    if (warmup) {
      console.error('[Test Validator] Running warmup iterations...');
    }

    const suites = testSuite ? [testSuite] : ['api-endpoints', 'database-queries', 'file-operations'];
    const results: PerformanceResult[] = [];

    for (const suite of suites) {
      const result = this.generateMockPerformanceResult(suite, iterations);
      results.push(result);
    }

    const avgThroughput = results.reduce((sum, r) => sum + r.throughput, 0) / results.length;
    const maxMemoryUsage = Math.max(...results.map(r => r.memoryUsage));

    const recommendations = this.generatePerformanceRecommendations(results);

    return {
      results,
      summary: {
        totalSuites: results.length,
        avgThroughput,
        maxMemoryUsage,
        recommendations
      }
    };
  }

  /**
   * ç”Ÿæˆæ¨¡æ“¬æ€§èƒ½çµæœ
   */
  private generateMockPerformanceResult(testSuite: string, iterations: number): PerformanceResult {
    const baseTime = Math.random() * 100 + 50;
    const avgTime = baseTime + (Math.random() * 20 - 10);
    const minTime = avgTime * 0.8;
    const maxTime = avgTime * 1.5;
    const throughput = Math.floor(1000 / avgTime);
    const memoryUsage = Math.floor(Math.random() * 100 + 50);

    return {
      testSuite,
      iterations,
      avgTime: Math.round(avgTime * 100) / 100,
      minTime: Math.round(minTime * 100) / 100,
      maxTime: Math.round(maxTime * 100) / 100,
      throughput,
      memoryUsage
    };
  }

  /**
   * ç”Ÿæˆæ€§èƒ½å»ºè­°
   */
  private generatePerformanceRecommendations(results: PerformanceResult[]): string[] {
    const recommendations = [];

    const slowSuites = results.filter(r => r.avgTime > 100);
    if (slowSuites.length > 0) {
      recommendations.push(`å„ªåŒ–æ€§èƒ½è¼ƒæ…¢çš„æ¸¬è©¦å¥—ä»¶: ${slowSuites.map(s => s.testSuite).join(', ')}`);
    }

    const highMemorySuites = results.filter(r => r.memoryUsage > 80);
    if (highMemorySuites.length > 0) {
      recommendations.push(`æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨é‡éé«˜çš„å¥—ä»¶: ${highMemorySuites.map(s => s.testSuite).join(', ')}`);
    }

    const lowThroughputSuites = results.filter(r => r.throughput < 5);
    if (lowThroughputSuites.length > 0) {
      recommendations.push(`æå‡ä½ååé‡å¥—ä»¶çš„æ€§èƒ½: ${lowThroughputSuites.map(s => s.testSuite).join(', ')}`);
    }

    if (recommendations.length === 0) {
      recommendations.push('æ€§èƒ½è¡¨ç¾è‰¯å¥½ï¼Œç¹¼çºŒç›£æ§é—œéµæŒ‡æ¨™');
    }

    recommendations.push('è€ƒæ…®è¨­ç½®æ€§èƒ½å›æ­¸æ¸¬è©¦');
    recommendations.push('å®šæœŸé€²è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦');

    return recommendations;
  }

  /**
   * ç²å– AI æ´å¯Ÿ
   */
  async getAIInsight(query: string): Promise<string> {
    console.error(`[Test Validator] Processing AI insight query: ${query}`);
    
    return `åŸºæ–¼æŸ¥è©¢ "${query}"ï¼Œæ¸¬è©¦å»ºè­°ï¼šä¿æŒé«˜æ¸¬è©¦è¦†è“‹ç‡ã€å®šæœŸåŸ·è¡Œæ€§èƒ½æ¸¬è©¦ã€è‡ªå‹•åŒ–æ¸¬è©¦æµç¨‹ã€‚`;
  }
}

const testValidator = new VibeTestValidator();
const server = new Server(
  { name: 'vibecoding-test-validator', version: '1.0.0' },
  { capabilities: { tools: {} } }
);

server.setRequestHandler(ListToolsRequestSchema, async () => ({
  tools: [
    {
      name: 'start-session',
      description: 'Start testing session',
      inputSchema: {
        type: 'object',
        properties: { projectId: { type: 'string' } }
      }
    },
    {
      name: 'run-tests',
      description: 'Execute test suites and return detailed results',
      inputSchema: {
        type: 'object',
        properties: {
          projectPath: { type: 'string' },
          testType: { type: 'string', enum: ['unit', 'integration', 'e2e', 'all'] },
          pattern: { type: 'string' },
          watch: { type: 'boolean' }
        },
        required: ['projectPath']
      }
    },
    {
      name: 'generate-test-report',
      description: 'Generate comprehensive test reports with metrics',
      inputSchema: {
        type: 'object',
        properties: {
          projectPath: { type: 'string' },
          format: { type: 'string', enum: ['html', 'json', 'xml', 'lcov'] },
          includeMetrics: { type: 'boolean' }
        },
        required: ['projectPath']
      }
    },
    {
      name: 'validate-coverage',
      description: 'Validate test coverage against thresholds',
      inputSchema: {
        type: 'object',
        properties: {
          projectPath: { type: 'string' },
          threshold: {
            type: 'object',
            properties: {
              lines: { type: 'number' },
              functions: { type: 'number' },
              branches: { type: 'number' },
              statements: { type: 'number' }
            }
          },
          failOnThreshold: { type: 'boolean' }
        },
        required: ['projectPath']
      }
    },
    {
      name: 'performance-test',
      description: 'Run performance tests and benchmarks',
      inputSchema: {
        type: 'object',
        properties: {
          projectPath: { type: 'string' },
          testSuite: { type: 'string' },
          iterations: { type: 'number' },
          warmup: { type: 'boolean' }
        },
        required: ['projectPath']
      }
    },
    {
      name: 'get-ai-insight',
      description: 'Get AI insights',
      inputSchema: {
        type: 'object',
        properties: { query: { type: 'string' } },
        required: ['query']
      }
    }
  ]
}));

// MCP åƒæ•¸é¡å‹è½‰æ›åŠ©æ‰‹
function extractParam<T>(args: unknown, key: string, defaultValue: T): T {
  if (args && typeof args === 'object' && args !== null) {
    const value = (args as Record<string, unknown>)[key];
    return value !== undefined ? value as T : defaultValue;
  }
  return defaultValue;
}

function extractStringParam(args: unknown, key: string, defaultValue: string = ''): string {
  return extractParam(args, key, defaultValue);
}

function extractOptionalStringParam(args: unknown, key: string): string | undefined {
  const value = extractParam(args, key, undefined);
  return value as string | undefined;
}

function extractObjectParam<T extends Record<string, any>>(args: unknown, key: string): T | undefined {
  const value = extractParam(args, key, undefined);
  return value && typeof value === 'object' ? value as T : undefined;
}

// è™•ç†å·¥å…·èª¿ç”¨
server.setRequestHandler(CallToolRequestSchema, async (request) => {
  try {
    const { name, arguments: args } = request.params;

    switch (name) {
      case 'start-session':
        return { content: [{ type: 'text', text: JSON.stringify(await testValidator.startSession(), null, 2) }] };
      case 'run-tests':
        return { content: [{ type: 'text', text: JSON.stringify(await testValidator.runTests(extractStringParam(args, 'projectPath', '.'), extractParam(args, 'testType', 'unit'), extractOptionalStringParam(args, 'pattern'), extractParam(args, 'watch', false)), null, 2) }] };
      case 'generate-test-report':
        return { content: [{ type: 'text', text: JSON.stringify(await testValidator.generateTestReport(extractStringParam(args, 'projectPath', '.'), extractParam(args, 'format', 'html'), extractParam(args, 'includeMetrics', true)), null, 2) }] };
      case 'validate-coverage':
        return { content: [{ type: 'text', text: JSON.stringify(await testValidator.validateCoverage(extractStringParam(args, 'projectPath', '.'), extractObjectParam(args, 'threshold') || {}, extractParam(args, 'failOnThreshold', false)), null, 2) }] };
      case 'performance-test':
        return { content: [{ type: 'text', text: JSON.stringify(await testValidator.performanceTest(extractStringParam(args, 'projectPath', '.'), extractOptionalStringParam(args, 'testSuite'), extractParam(args, 'iterations', 100), extractParam(args, 'warmup', true)), null, 2) }] };
      case 'get-ai-insight':
        return { content: [{ type: 'text', text: await testValidator.getAIInsight(extractStringParam(args, 'query')) }] };
      default:
        throw new McpError(ErrorCode.MethodNotFound, `Unknown tool: ${name}`);
    }
  } catch (error) {
    console.error('[Test Validator] Tool execution error:', error);
    throw new McpError(ErrorCode.InternalError, `Tool execution failed: ${error instanceof Error ? error.message : String(error)}`);
  }
});

async function runServer() {
  const transport = new StdioServerTransport();
  console.error('ğŸ¯ VibeCoding Test Validator MCP Server starting...');
  console.error('ğŸ“‹ Prompt system integration: ENABLED');
  await server.connect(transport);
}

runServer().catch(console.error); 